{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(image, cmap=None):\n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.imshow(image, cmap=cmap)\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def region_of_interest(image, vertices):\n",
    "    # blank mask\n",
    "    mask = np.zeros_like(image)   \n",
    "    \n",
    "    # 3 or more channels\n",
    "    if len(image.shape) > 2:\n",
    "        channel_count = image.shape[2]\n",
    "        mask_color = (255,) * channel_count\n",
    "    # 1 channel\n",
    "    else:\n",
    "        mask_color = 255\n",
    "        \n",
    "    # keep the pixels inside the region defined by vertices   \n",
    "    # their color = mask_color\n",
    "    cv2.fillPoly(mask, vertices, mask_color)\n",
    "    \n",
    "    # keep only the region of interest\n",
    "    return cv2.bitwise_and(image, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_image(initial_image, image, alpha, beta, gamma):\n",
    "    # res = initial_image * alpha + image * beta + gamma\n",
    "    return cv2.addWeighted(initial_image, alpha, image, beta, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_grayscale(image):\n",
    "    return cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_slope(line):\n",
    "    x1, y1, x2, y2 = line\n",
    "    \n",
    "    return np.inf if x1 == x2 else (y2-y1)/(x2-x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constructs a new blank image and fills it with the specified lines\n",
    "def fill_with_lines(lines, image_shape, thickness=5):\n",
    "    img_lines = np.zeros(image_shape, dtype=np.uint8)\n",
    "\n",
    "    for line in lines:\n",
    "        for x1, y1, x2, y2 in line:\n",
    "            cv2.line(img_lines, (x1,y1), (x2,y2), (255, 0, 0), thickness)\n",
    "    \n",
    "    return img_lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Basic approach: simple lane identification by color selection and region of interest\n",
    "\n",
    "First step: color selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the image\n",
    "image = mpimg.imread('./imgs/lanes1.jpeg')\n",
    "\n",
    "show_image(image)\n",
    "\n",
    "# Grab the x and y size and make a copy of the image\n",
    "ysize = image.shape[0]\n",
    "xsize = image.shape[1]\n",
    "\n",
    "img_color_select = np.copy(image)\n",
    "\n",
    "# Define color selection criteria\n",
    "thr = 195\n",
    "red_threshold = thr\n",
    "green_threshold = thr\n",
    "blue_threshold = thr\n",
    "\n",
    "rgb_threshold = [red_threshold, green_threshold, blue_threshold]\n",
    "\n",
    "# Identify pixels below the thresholds\n",
    "color_thresholds = (image[:,:,0] < rgb_threshold[0]) \\\n",
    "            | (image[:,:,1] < rgb_threshold[1]) \\\n",
    "            | (image[:,:,2] < rgb_threshold[2])\n",
    "img_color_select[color_thresholds] = [0, 0, 0]\n",
    "\n",
    "# Display the image   \n",
    "show_image(img_color_select)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second step: assume that the camera is mounted in a **fixed** position on the front of the car => we can derive a **region of interest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a triangle region of interest \n",
    "ybias = 0.05 * ysize\n",
    "xbias = 0.05 * xsize\n",
    "\n",
    "apex = [xsize / 2, ysize / 2 + ybias]\n",
    "left_bottom = [xbias, ysize]\n",
    "right_bottom = [xsize - xbias, ysize]\n",
    "\n",
    "# Draw region of interest\n",
    "x = [left_bottom[0], right_bottom[0], apex[0], left_bottom[0]]\n",
    "y = [left_bottom[1], right_bottom[1], apex[1], left_bottom[1]]\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(x, y, 'b--', lw=4)\n",
    "plt.imshow(image)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine color selection with region of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_color_select = np.copy(image)\n",
    "img_line_select = np.copy(image)\n",
    "\n",
    "fit_left = np.polyfit((left_bottom[0], apex[0]), (left_bottom[1], apex[1]), 1)\n",
    "fit_right = np.polyfit((right_bottom[0], apex[0]), (right_bottom[1], apex[1]), 1)\n",
    "fit_bottom = np.polyfit((left_bottom[0], right_bottom[0]), (left_bottom[1], right_bottom[1]), 1)\n",
    "\n",
    "# Find the region inside the lines\n",
    "XX, YY = np.meshgrid(np.arange(0, xsize), np.arange(0, ysize))\n",
    "region_thresholds = (YY > (XX*fit_left[0] + fit_left[1])) & \\\n",
    "                    (YY > (XX*fit_right[0] + fit_right[1])) & \\\n",
    "                    (YY < (XX*fit_bottom[0] + fit_bottom[1]))\n",
    "\n",
    "# Mask color and region selection\n",
    "img_color_select[color_thresholds | ~region_thresholds] = [0, 0, 0]\n",
    "# Color pixels red where both color and region selections met\n",
    "img_line_select[~color_thresholds & region_thresholds] = [255, 0, 0]\n",
    "\n",
    "\n",
    "# Draw triangle (region of interest)\n",
    "x = [left_bottom[0], right_bottom[0], apex[0], left_bottom[0]]\n",
    "y = [left_bottom[1], right_bottom[1], apex[1], left_bottom[1]]\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(x, y, 'b--', lw=4)\n",
    "plt.imshow(img_color_select)\n",
    "plt.show()\n",
    "\n",
    "# draw recognized lines in red\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.imshow(img_line_select)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Better approach: Canny edge detection + Hough-space transform\n",
    "\n",
    "Canny: identify the boundaries of an object in an image.\n",
    "- convert to grayscale\n",
    "- compute the gradient (in _gradient space_, the brightness of a pixel corresponds to the strength of the gradient at that point)\n",
    "- find edges by tracing the pixels that follow the strongest gradients\n",
    "\n",
    "`cv2.Canny(image, low_threshold, high_threshold)`\n",
    "\n",
    "`low_threshold` and `high_threshold` determine how strong the edges must be to be detected (= difference of values of adjacent pixels = strength of the gradient)\n",
    "\n",
    "Canny keeps strong edge (strong gradient) pixels above the `high_threshold`, and reject pixels below the `low_threshold`. Pixels with values between the `low_threshold` and `high_threshold` will be included as long as they are connected to strong edges. \n",
    "\n",
    "edge = rapid change in brightness\n",
    "\n",
    "Canny gives the individual pixels that follow the strongest gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def canny(image, low, high, gaussian=False):\n",
    "    if gaussian:\n",
    "        kernel_size = 5\n",
    "        image = cv2.GaussianBlur(image, (kernel_size, kernel_size), 0)\n",
    "    \n",
    "    img_edges = cv2.Canny(image, low, high)\n",
    "    return img_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread(\"./imgs/exit-ramp.jpg\")\n",
    "img_gray = to_grayscale(image)\n",
    "\n",
    "show_image(img_gray, 'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without extra gaussian filter\n",
    "img_edges = canny(img_gray, 50, 150)\n",
    "\n",
    "show_image(img_edges, 'Greys_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With extra gaussian filter\n",
    "img_edges = canny(img_gray, 50, 150, True)\n",
    "\n",
    "show_image(img_edges, 'Greys_r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hough Space** is parameter space (in our case, $m$ and $b$).\n",
    "\n",
    "- a line in IS becomes a point in HS: $y = m_0 x + b_0 \\rightarrow (m_0, b_0)$\n",
    "- a point in IS becomes a line in HS: $(x_0, y_0) \\rightarrow m = \\frac{y_0}{x_0} - \\frac{b}{x_0}$\n",
    "\n",
    "For 2 points $p_1, p_2$ in IS there are 2 **intersecting** lines in HS. The intersection point is at $(m, b)$ that defines the single line that passes through both $p_1$ and $p_2$.\n",
    "\n",
    "So, the strategy to find lines in IS is to look for intersecting lines in HS. We can consider every point in the `img_edges` as a line in HS and where many lines in HS intersect => found a collection of points that describe a line in IS.\n",
    "\n",
    "*Problem:* vertical lines have $m = \\infty$ => new parameterization.\n",
    "\n",
    "IS: $\\rho_0 = x \\cos(\\theta_0) + y \\sin(\\theta_0)$\n",
    "\n",
    "HS: $(\\theta_0, \\rho_0)$\n",
    "\n",
    "Now, each point in IS corresponds to a sine curve in HS. The intersection of the sine curves in HS gives $(\\theta_0, \\rho_0)$, the paremeterization of the line in IS.\n",
    "\n",
    "**Grid layout** resolution: $\\theta_r$ radians, $\\rho_r$ pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct trapezoidal region\n",
    "def construct_region(img_shape):\n",
    "    [ysize, xsize, _] = img_shape\n",
    "    \n",
    "    dx = 0.07 * xsize\n",
    "    dy = 0.52 * ysize\n",
    "    gap = 20\n",
    "    \n",
    "    p1 = [dx, ysize]\n",
    "    p2 = [xsize - dx, ysize]\n",
    "    p3 = [xsize / 2 + gap / 2, dy]\n",
    "    p4 = [xsize / 2 - gap / 2, dy]\n",
    "    \n",
    "    trapezoidal_region = np.array([p1, p2, p3, p4], dtype=np.int32)\n",
    "    \n",
    "    return trapezoidal_region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv2.HoughLinesP returns an array \n",
    "# containing the endpoints (x1, y1, x2, y2) \n",
    "# of all line segments detected by the transform operation\n",
    "\n",
    "# pixels\n",
    "rho_res = 2\n",
    "# degrees\n",
    "theta_res = 1 * np.pi / 180 \n",
    "# minimum number of votes (intersections in a given grid cell) \n",
    "# a candidate line needs to have to make it into the output\n",
    "threshold = 15\n",
    "# minimum length of a line (in pixels) that will be accepted in the output\n",
    "min_line_length = 40\n",
    "# maximum distance (in pixels) between segments that will be allowed to be connected into a single line\n",
    "max_line_gap = 20\n",
    "\n",
    "# create a blank image (same size as original image)\n",
    "lines = cv2.HoughLinesP(img_edges, rho_res, theta_res, threshold, np.array([]), min_line_length, max_line_gap)\n",
    "\n",
    "img_lines = fill_with_lines(lines, image.shape)\n",
    "\n",
    "# construct trapezoidal region\n",
    "trapezoidal_region = construct_region(image.shape)\n",
    "\n",
    "color_edges = np.dstack((img_edges,) * 3)\n",
    "\n",
    "# get only those lines inside the trapezoidal region\n",
    "constrained_lines = region_of_interest(img_lines, [trapezoidal_region])\n",
    "\n",
    "# out = 0.8 * color_edges + 1.0 * img_lines + 0\n",
    "out = weighted_image(color_edges, constrained_lines, 0.8, 1, 0)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "\n",
    "xs = [trapezoidal_region[i][0] for i in np.arange(5) % 4]\n",
    "ys = [trapezoidal_region[i][1] for i in np.arange(5) % 4]\n",
    "\n",
    "plt.plot(xs, ys, 'b--', lw=2)\n",
    "\n",
    "plt.imshow(out)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect(image_path, debug=False):\n",
    "    image = mpimg.imread(image_path)\n",
    "    img_gray = to_grayscale(image)\n",
    "    img_edges = canny(img_gray, 125, 200, True)\n",
    "    \n",
    "    # parameters\n",
    "    rho = 2\n",
    "    theta = 1 * np.pi / 180 \n",
    "    threshold = 15\n",
    "    min_line_length = 40\n",
    "    max_line_gap = 20\n",
    "    \n",
    "    lines = cv2.HoughLinesP(img_edges, rho, theta, threshold, np.array([]), min_line_length, max_line_gap)\n",
    "    \n",
    "    #img_lines = fill_with_lines(lines, image.shape, 2)\n",
    "    img_lines = np.zeros_like(image)\n",
    "    s = []\n",
    "    for line in lines:\n",
    "        slope = get_slope(line[0])\n",
    "        \n",
    "        if slope != np.inf:\n",
    "            deg = np.abs(slope * 180 / np.pi)\n",
    "            if 25 <= deg <= 45:\n",
    "                s.append(slope * 180 / np.pi)\n",
    "                \n",
    "                for x1, y1, x2, y2 in line:\n",
    "                    cv2.line(img_lines, (x1,y1), (x2,y2), (255, 0, 0), 5)\n",
    "    \n",
    "    \n",
    "            \n",
    "\n",
    "    # construct trapezoidal region\n",
    "    trapezoidal_region = construct_region(image.shape)\n",
    "\n",
    "    color_edges = np.dstack((img_edges,) * 3)\n",
    "\n",
    "    # get only those lines inside the trapezoidal region\n",
    "    constrained_lines = region_of_interest(img_lines, [trapezoidal_region])\n",
    "\n",
    "    # out = 0.8 * color_edges + 1.0 * img_lines + 0\n",
    "    out = weighted_image(image, constrained_lines, 0.8, 1, 0)\n",
    "    \n",
    "    plt.figure(figsize=(20,10))\n",
    "    \n",
    "    if debug:\n",
    "        out = weighted_image(color_edges, constrained_lines, 0.8, 1, 0)\n",
    "        \n",
    "        xs = [trapezoidal_region[i][0] for i in np.arange(5) % 4]\n",
    "        ys = [trapezoidal_region[i][1] for i in np.arange(5) % 4]\n",
    "\n",
    "        plt.plot(xs, ys, 'b--', lw=2)\n",
    "        \n",
    "    plt.imshow(out)\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "    return out, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = os.listdir(\"./imgs\")\n",
    "\n",
    "for img in imgs:\n",
    "    i, s = detect(\"./imgs/\" + img, True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
